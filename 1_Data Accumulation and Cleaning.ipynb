{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # dataframe manipulation\n",
    "import numpy as np # data manipulation\n",
    "import lucem_illud_2020 # for word tokenizing\n",
    "import pickle # data storage and retrieval\n",
    "from collections import defaultdict # for handling dictionary more easily\n",
    "import matplotlib.pyplot as plt # For plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulating the data\n",
    "\n",
    "Note that I do not include the any of the data in the repository for copyright reasons.\n",
    "\n",
    "## Publication data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the abstracts data\n",
    "abstracts = pd.read_csv('rawdata/abstracts_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nousl\\Anaconda3\\envs\\macs60000\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# initialize a empty dataframe to load all the data\n",
    "df = pd.DataFrame(columns=['wos_id', 'title', 'pubyear', 'journal'])\n",
    "df = df.astype({'wos_id':str, 'title':str, 'pubyear':np.int32, 'journal':str})\n",
    "\n",
    "# collect all publication data in `rawdata/publications`\n",
    "for i in range(1985, 2016):\n",
    "    tmp_df = pd.read_csv(f'rawdata/publications/publications.{i}.results.tsv', sep='\\t', skiprows=1, quoting=3)\n",
    "    # subset only the relevant columns and drop duplicates\n",
    "    tmp_df = tmp_df[['wos_id', 'title', 'pubyear', 'journal']].copy().drop_duplicates().reset_index(drop=True)\n",
    "    tmp_df = tmp_df.astype({'wos_id':str, 'title':str, 'pubyear':np.int32})\n",
    "    df = pd.concat((df, tmp_df))\n",
    "# free up memory\n",
    "del tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of the original data was 1061195\n"
     ]
    }
   ],
   "source": [
    "print(f'the size of the original data was {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the publication data with the abstracts\n",
    "merged_df = df.merge(abstracts, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up some memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop publication infos without valid abstracts\n",
    "merged_df = merged_df[(merged_df['abstract'] != '()')].copy()\n",
    "# clean up the abstract\n",
    "merged_df['abstract'] = merged_df['abstract'].apply(lambda x: x.strip('((').strip(',),').strip('\"').strip(\"'\").strip(','))\n",
    "# concatenate the title and abstract to form \"text\"\n",
    "merged_df['text'] = merged_df['title'] + \" \" + merged_df['abstract']\n",
    "merged_df = merged_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of the data with valid abstract was 579776\n",
      "the ratio of retained data was 0.5463425666347844\n"
     ]
    }
   ],
   "source": [
    "print(f'the size of the data with valid abstract was {merged_df.shape[0]}')\n",
    "print(f'the ratio of retained data was {merged_df.shape[0] / df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tokenizing the text\n",
    "merged_df['tokenized_words'] = merged_df['text'].apply(lambda x: lucem_illud_2020.word_tokenize(x))\n",
    "# normalizing the text without lemmatizing (to use for building doc2vec model)\n",
    "merged_df['normalized_words'] = merged_df['tokenized_words'].apply(lambda x: lucem_illud_2020.normalizeTokens(x, lemma=False))\n",
    "# normalizing text in a standard fashion (to use for other purposes)\n",
    "merged_df['normalized_words_standard'] = merged_df['tokenized_words'].apply(lambda x: lucem_illud_2020.normalizeTokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the regular text to decrease file size\n",
    "merged_df.drop(columns=['title', 'abstract', 'text'], inplace=True)\n",
    "merged_df.to_pickle('data/tokenized_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author data\n",
    "\n",
    "getting all the author data from the raw data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a empty dataframe to load all the data\n",
    "df = pd.DataFrame(columns=['wos_id', 'AuthorID', 'wos_standard', 'email_addr'])\n",
    "df = df.astype({'wos_id':str, 'AuthorID':float, 'wos_standard':str, 'email_addr':str})\n",
    "\n",
    "# collect all publication data in `rawdata/authors`\n",
    "for i in range(1985, 2016):\n",
    "    author_df = pd.read_csv(f'rawdata/authors/authors.{i}.results.tsv', sep='\\t', skiprows=1, quoting = 3)\n",
    "    # drop authors that do not have a valid author ID nor a valid email address\n",
    "    author_df = author_df[~(author_df['AuthorID'].isna()) | ~(author_df['email_addr'].isna())]\n",
    "    # drop columns that is not used for analysis\n",
    "    author_df.drop(columns=['position', 'display_name'], inplace=True)\n",
    "    author_df = author_df.astype({'wos_id':str, 'AuthorID':float, 'wos_standard':str, 'email_addr':str})\n",
    "    df = pd.concat((df, author_df))\n",
    "# free up some memory\n",
    "del author_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Web of Science Author IDs based on e-mail\n",
    "\n",
    "WOS has their own 'author ID' that are unique to all authors, so I linked the emails from the survey to the author IDs. But the problem is that (1) not all authors have an author IDs, (2) some authors have more than one WOS ID, (3) there are some mistakes in the WOS database. So to deal with this situation, I followed these rules:\n",
    "\n",
    "Rules:\n",
    "(1) For all e-mails that has a unique author ID in the data, the unique author ID is considered to be the author ID for that e-mail\n",
    "(2) If the e-mail have more than one author ID attatched to it, compare the name in the survey and the name in the database, whatever matches the name are condsidered to be the author ID(s) for the e-mail\n",
    "(3) If the e-mail have more than one author ID attatched to it, and none of the names in the database match the name in the survey, manually inspect it and make a decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the author names in the df to match the survey names\n",
    "df['wos_standard'] = df['wos_standard'].apply(lambda x: x.lower().split(',')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the emails from the survey data\n",
    "link_email_df = pd.read_csv('rawdata/survey/linkEmails.csv')\n",
    "# drop irrelevant columns\n",
    "link_email_df = link_email_df[['email', 'anonKey', 'name']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to numpy array for faster calculation\n",
    "link_email_np = link_email_df.to_numpy()\n",
    "author_np = df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I ran the cell below but deleted in the uploaded version on the repository because of privacy reasons. There were two names/emails that I manually checked, but I ended up not including both of them just to be cautious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# initiallize an empty dictionary to store the results\n",
    "email_wos_author_dict = {}\n",
    "for email in link_email_np[:, 0]:\n",
    "    # subset all rows that have a matching e-mail\n",
    "    tmp = author_np[author_np[:, 3] == email]\n",
    "    tmp = tmp[~np.isnan(tmp[:, 1].astype(float))] # subset all authors with author IDs\n",
    "    # if there is no author ID for that email, skip it\n",
    "    if tmp.size == 0:\n",
    "        continue\n",
    "    \n",
    "    # get the unique list of author IDs and names\n",
    "    wos_lst = list(np.unique(tmp[:, 1]).astype(int))\n",
    "    name_lst = list(np.unique(tmp[:, 2]).astype(str))\n",
    "    # get the name in the survey\n",
    "    survey_name = link_email_np[link_email_np[:, 0] == email][0, 2]\n",
    "    \n",
    "    # change the name in the survey to lower case\n",
    "    # the if clause is for cases where the name is nan\n",
    "    if type(survey_name) != float:\n",
    "        survey_name = survey_name.lower()\n",
    "\n",
    "    # if there is only one author ID, then we use that WOS ID\n",
    "    if len(wos_lst) == 1:\n",
    "        new_wos_lst = wos_lst\n",
    "\n",
    "    # if there are more than one author ID, we keep the ones that have the matching names\n",
    "    elif len(wos_lst) > 1 and len(name_lst) > 1:\n",
    "        new_wos_lst = set()\n",
    "        for row in tmp:\n",
    "            if row[2] == survey_name:\n",
    "                new_wos_lst.add(row[1])\n",
    "        new_wos_lst = list(new_wos_lst)\n",
    "        \n",
    "        # if there are more than one author ID but none of the name matches,\n",
    "        # I will manually inspect the data and make a decision\n",
    "        if len(new_wos_lst) == 0:\n",
    "            print('there are more than one WOS ID attached to the email but none of them match survey names:')\n",
    "            print(f'survey_name: {survey_name}')\n",
    "            print(np.unique(tmp[:, 1:].astype(str), axis=0))\n",
    "            print('----')\n",
    "    else:\n",
    "        new_wos_lst = wos_lst\n",
    "    email_wos_author_dict[email] = new_wos_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the articles written by survey respondents\n",
    "\n",
    "For all survey respondents that had at least one articles with matching e-mails, I will create a dictionary to store the relationship. For convienience, the two dictionaries are created: one using key as the annonimized author ID (different from WOS author IDs) and values with the WOS ID of the related article, and another one where the key-value is reversed.\n",
    "\n",
    "In addition to articles with matching e-mails, articles that matches the WOS author ID identified above will also be included.\n",
    "\n",
    "Note that this dictionaries includes documents that does not have valid abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries\n",
    "doc_annon_dict = defaultdict(list)\n",
    "annon_doc_dict = defaultdict(list)\n",
    "\n",
    "for email in link_email_np[:, 0]:\n",
    "    # if there was no email, I can't do anything\n",
    "    if type(email) == float:\n",
    "        continue\n",
    "    # get the annonimized author ID\n",
    "    annon = link_email_np[link_email_np[:, 0] == email][0, 1]\n",
    "    # get the set of articles that has the matching email\n",
    "    wos_set = set(np.unique(author_np[author_np[:,3] == email][:, 0].astype(str)))\n",
    "    # if the email has attached WOS author ID(s), collect articles with that author IDs, too\n",
    "    for wos_author_id in email_wos_author_dict.get(email, []):\n",
    "        wos_set = wos_set.union(set(np.unique(author_np[author_np[:,1] == wos_author_id][:,0].astype(str))))\n",
    "    # save the dictionaries\n",
    "    for wos_id in wos_set:\n",
    "        doc_annon_dict[wos_id].append(annon)\n",
    "    if wos_set:\n",
    "        annon_doc_dict[annon] = list(wos_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionaries\n",
    "with open('data/doc_annon_dict', 'wb') as f:\n",
    "    pickle.dump(doc_annon_dict, f)\n",
    "with open('data/annon_doc_dict', 'wb') as f:\n",
    "    pickle.dump(annon_doc_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the number of documents that have valid abstracts attached to each author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataframe to store the data\n",
    "author_documents_count = pd.DataFrame()\n",
    "# iterate through the dictionary to get the number of documents for each author\n",
    "idx = 0\n",
    "for author, doc in annon_doc_dict.items():\n",
    "    author_documents_count.loc[idx, 'author'] = author\n",
    "    author_documents_count.loc[idx, 'docnum'] = len(doc)\n",
    "    if len(doc) > 0:\n",
    "        # get the number of documents that are also in the dictionary, i.e. documents with valid abstracts attached\n",
    "        author_documents_count.loc[idx, 'abstract_docnum'] = np.isin(annon_doc_dict[author], merged_df['wos_id']).sum()\n",
    "    else:\n",
    "        author_documents_count.loc[idx, 'abstract_docnum'] = 0\n",
    "    idx += 1\n",
    "# only keep the authors that have at least one article with valid abstract\n",
    "author_documents_count = author_documents_count[author_documents_count['abstract_docnum'] > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_documents_count.to_csv('data/author_documents_count.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average number of valid doucments for author is: 7.1856016110085585\n",
      "the median is: 2.0\n",
      "the standard deviation is: 14.319888332292118\n",
      "the variance is: 205.05920184931594\n"
     ]
    }
   ],
   "source": [
    "print(f'the average number of valid doucments for author is: {author_documents_count[\"abstract_docnum\"].mean()}')\n",
    "print(f'the median is: {author_documents_count[\"abstract_docnum\"].median()}')\n",
    "print(f'the standard deviation is: {author_documents_count[\"abstract_docnum\"].std()}')\n",
    "print(f'the variance is: {author_documents_count[\"abstract_docnum\"].var()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the number of documents that have at least one author attached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40830"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(doc_annon_dict) & set(df['wos_id']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
